# -*- coding: utf-8 -*-
"""Emotion Model Report Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1okAzpgeCzuVSO6qhWQKWrfpebg5vTAax
"""

# -*- coding: utf-8 -*-
"""
Emotion Model Performance Report Generator (Corrected)

This script loads a pre-trained emotion detection model, regenerates the
exact same test dataset using the original data processing logic, and
evaluates the model's performance.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
import numpy as np
import random
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (confusion_matrix, classification_report,
                           accuracy_score, precision_score, recall_score)
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from collections import Counter, defaultdict
import uuid
import os
import subprocess
import re

# Set a seed for reproducibility to ensure the data split is the same
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# --- Dependency Handling ---
def safe_import(package_name, install_command=None):
    try:
        if package_name == "whisper":
            import whisper
            return whisper, True
        elif package_name == "librosa":
            import librosa
            return librosa, True
    except ImportError:
        print(f"'{package_name}' not found.")
        if install_command:
            user_input = input(f"Would you like to try installing it now? (y/n): ").lower()
            if user_input == 'y':
                try:
                    print(f"Installing {package_name}...")
                    subprocess.check_call(install_command)
                    if package_name == "whisper":
                        import whisper
                        return whisper, True
                    elif package_name == "librosa":
                        import librosa
                        return librosa, True
                except Exception as e:
                    print(f"Could not install {package_name}: {e}")
        print(f"Please install '{package_name}' manually to proceed.")
        return None, False

whisper, WHISPER_AVAILABLE = safe_import("whisper", ["pip", "install", "openai-whisper"])
librosa, LIBROSA_AVAILABLE = safe_import("librosa", ["pip", "install", "librosa"])


# --- Class Definitions from Training Script ---
# These are identical to the training script to ensure compatibility.

@dataclass
class EnhancedConfig:
    """Enhanced configuration for 40% target performance"""
    base_model: str = "roberta-large"
    hidden_size: int = 1024
    emotion_classes: List[str] = field(default_factory=lambda: ["Anger", "Disgust", "Fear", "Happy", "Neutral", "Sad"])
    max_sequence_length: int = 256
    dropout_rate: float = 0.1
    learning_rate: float = 1e-5
    batch_size: int = 8
    num_epochs: int = 30
    warmup_ratio: float = 0.2
    weight_decay: float = 0.01
    gradient_clip_norm: float = 1.0
    audio_test_ratio: float = 0.10
    audio_val_ratio: float = 0.10
    synthetic_samples_per_emotion: int = 800
    use_domain_adaptation: bool = True
    whisper_model_size: str = "large"
    use_multiple_transcriptions: bool = True
    audio_quality_threshold: float = 0.6
    use_focal_loss: bool = True
    focal_alpha: float = 1.0
    focal_gamma: float = 2.0
    audio_weight: float = 3.0
    use_mixup: bool = True
    mixup_alpha: float = 0.2
    use_label_smoothing: bool = True
    label_smoothing_factor: float = 0.1
    use_adversarial_training: bool = True
    use_audio_augmentation: bool = True
    audio_noise_factor: float = 0.005
    audio_pitch_shift_steps: int = 2
    use_multi_layer_pooling: bool = True
    use_attention_pooling: bool = True
    hidden_layers: List[int] = field(default_factory=lambda: [1024, 512, 256])
    use_cosine_schedule: bool = True
    use_ema: bool = True
    ema_decay: float = 0.999
    use_audio_transcription: bool = True
    sample_rate: int = 16000
    max_audio_length: float = 12.0
    min_text_length: int = 2
    use_audio_preprocessing: bool = True
    crema_dataset_path: str = "./crema_d_data"
    ravdess_dataset_path: str = "./ravdess_data"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

class EnhancedModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(config.base_model)
        self.backbone = AutoModel.from_pretrained(config.base_model)
        if config.use_multi_layer_pooling:
            self.layer_weights = nn.Parameter(torch.ones(4))
        if config.use_attention_pooling:
            self.attention = nn.MultiheadAttention(config.hidden_size, 8, 0.1, batch_first=True)
            self.attention_norm = nn.LayerNorm(config.hidden_size)
        layers, input_size = [], config.hidden_size
        for hidden_dim in config.hidden_layers:
            layers.extend([nn.Linear(input_size, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.GELU(), nn.Dropout(config.dropout_rate)])
            input_size = hidden_dim
        layers.append(nn.Linear(input_size, len(config.emotion_classes)))
        self.classifier = nn.Sequential(*layers)

    def forward(self, input_ids=None, attention_mask=None):
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=self.config.use_multi_layer_pooling)
        if self.config.use_multi_layer_pooling:
            hidden_states = outputs.hidden_states[-4:]
            layer_weights = F.softmax(self.layer_weights, dim=0)
            sequence_output = sum(weight * state for weight, state in zip(layer_weights, hidden_states))
        else:
            sequence_output = outputs.last_hidden_state
        if self.config.use_attention_pooling:
            attn_out, _ = self.attention(sequence_output, sequence_output, sequence_output, key_padding_mask=~attention_mask.bool())
            attn_out = self.attention_norm(attn_out + sequence_output)
            mask = attention_mask.unsqueeze(-1).float()
            pooled = (attn_out * mask).sum(dim=1) / mask.sum(dim=1)
        else:
            pooled = sequence_output[:, 0, :]
        logits = self.classifier(pooled)
        return {'logits': logits}

class EnhancedDataset(Dataset):
    def __init__(self, data, tokenizer, config):
        self.data, self.tokenizer, self.config = data, tokenizer, config
        self.emotion_to_id = {e: i for i, e in enumerate(config.emotion_classes)}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = ' '.join(item['text'].strip().split()) or f"sample from {item['emotion'].lower()}"
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.config.max_sequence_length, return_tensors='pt')
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(self.emotion_to_id[item['emotion']], dtype=torch.long)
        }

# --- REAL Data Processing (Copied 1-to-1 from training script) ---

class EnhancedSyntheticGenerator:
    """Full synthetic data generator to match training random state."""
    def __init__(self, config: EnhancedConfig):
        self.config = config
        self.audio_like_templates = {
            'Anger': ["I'm angry about this", "This makes me mad", "I hate this", "I'm so frustrated", "This is annoying", "I'm upset about this", "This bothers me", "I don't like this", "This makes me furious", "I'm irritated by this"],
            'Disgust': ["This is gross", "I find this disgusting", "This is nasty", "This makes me sick", "I hate this smell", "This is revolting", "This is awful", "I can't stand this", "This is terrible", "This disgusts me"],
            'Fear': ["I'm scared", "This frightens me", "I'm afraid", "This worries me", "I'm nervous", "This makes me anxious", "I'm concerned about this", "This scares me", "I feel worried", "This makes me uncomfortable"],
            'Happy': ["I'm happy", "This makes me feel good", "I love this", "This is great", "I'm excited", "This is wonderful", "I feel good about this", "This makes me smile", "I'm pleased with this", "This brings me joy"],
            'Neutral': ["This is fine", "I understand", "That's normal", "I see", "This is okay", "I'm listening", "That makes sense", "I agree", "This is usual", "I know"],
            'Sad': ["I'm sad", "This makes me feel bad", "I'm disappointed", "This hurts", "I feel down", "This is depressing", "I'm upset", "This makes me cry", "I feel terrible", "This is heartbreaking"]
        }
        self.complex_templates = {
            'Anger': ["I am absolutely furious about {situation}", "This makes me so angry I could {action}", "I hate it when {event} happens like this", "I'm livid about {problem} right now", "This is infuriating and {feeling}"],
            'Disgust': ["This is absolutely disgusting and {feeling}", "I find {thing} completely repulsive", "That {object} makes me sick to my stomach", "How revolting and {adjective} this is", "I'm nauseated by {situation}"],
            'Fear': ["I'm terrified of {situation} happening", "This scares me more than {comparison}", "I'm afraid that {event} will occur", "I feel anxious about {situation}", "This frightens me {intensifier}"],
            'Happy': ["I'm so happy about {situation}", "This brings me {feeling} and joy", "I feel wonderful about {event}", "I'm excited that {situation} happened", "This makes me smile {intensifier}"],
            'Neutral': ["I need to {action} the {object}", "The {event} is scheduled for {time}", "Please {action} the {document}", "The {meeting} will be held", "I have to {task} before {time}"],
            'Sad': ["I feel so sad about {situation}", "This makes me {feeling} and depressed", "I'm heartbroken about {loss}", "I feel down because of {problem}", "This brings tears to my eyes"]
        }
        self.variables = {
            'situation': ['work', 'home', 'this', 'that', 'everything'], 'action': ['scream', 'leave', 'stop', 'fix', 'change'], 'event': ['problems', 'issues', 'things', 'stuff', 'situations'],
            'problem': ['this mess', 'the situation', 'what happened', 'this issue'], 'feeling': ['wrong', 'bad', 'terrible', 'awful', 'horrible'], 'thing': ['this', 'that', 'it', 'everything'],
            'object': ['thing', 'situation', 'problem', 'issue'], 'adjective': ['bad', 'terrible', 'awful', 'horrible'], 'comparison': ['anything', 'everything', 'other things'],
            'intensifier': ['so much', 'a lot', 'completely'], 'time': ['today', 'now', 'soon', 'later'], 'task': ['do', 'finish', 'complete', 'handle'],
            'document': ['work', 'things', 'stuff'], 'meeting': ['thing', 'event', 'session'], 'loss': ['this', 'what happened', 'the situation']
        }

    def fill_template(self, template: str) -> str:
        text = template
        for var_type, options in self.variables.items():
            placeholder = f"{{{var_type}}}"
            if placeholder in text:
                text = text.replace(placeholder, random.choice(options))
        return re.sub(r'\{[^}]+\}', 'something', text)

    def generate_enhanced_synthetic(self) -> List[Dict]:
        synthetic_data = []
        for emotion in self.config.emotion_classes:
            audio_templates = self.audio_like_templates[emotion]
            complex_templates = self.complex_templates.get(emotion, audio_templates)
            for i in range(self.config.synthetic_samples_per_emotion):
                if i < self.config.synthetic_samples_per_emotion * 0.6:
                    text = random.choice(audio_templates)
                else:
                    template = random.choice(complex_templates)
                    text = self.fill_template(template)
                synthetic_data.append({'text': text, 'emotion': emotion, 'dataset': 'synthetic', 'data_type': 'synthetic', 'id': str(uuid.uuid4())})
        return synthetic_data

class EnhancedAudioProcessor:
    def __init__(self, config: EnhancedConfig):
        self.config = config
        self.crema_path = Path(config.crema_dataset_path)
        self.ravdess_path = Path(config.ravdess_dataset_path)
        self.crema_emotions = {'ANG': 'Anger', 'DIS': 'Disgust', 'FEA': 'Fear', 'HAP': 'Happy', 'NEU': 'Neutral', 'SAD': 'Sad'}
        self.ravdess_emotions = {1: 'Neutral', 2: 'Neutral', 3: 'Happy', 4: 'Sad', 5: 'Anger', 6: 'Fear', 7: 'Disgust', 8: 'Neutral'}
        if WHISPER_AVAILABLE and whisper:
            try:
                self.whisper_model = whisper.load_model(config.whisper_model_size)
            except Exception as e:
                print(f"Could not load Whisper model '{config.whisper_model_size}', falling back to 'base'. Error: {e}")
                self.whisper_model = whisper.load_model("base")
        else:
            self.whisper_model = None
        self.fallback_templates = {'Anger': ["I'm angry"], 'Disgust': ["This is gross"], 'Fear': ["I'm scared"], 'Happy': ["I'm happy"], 'Neutral': ["This is fine"], 'Sad': ["I'm sad"]}

    def enhanced_transcribe(self, audio_path: str, emotion: str) -> str:
        if not self.whisper_model: return random.choice(self.fallback_templates[emotion])
        try:
            # Note: The original trainer had more complex logic here. This is a simplified version for compatibility.
            result = self.whisper_model.transcribe(audio_path, language="en")
            text = result["text"].strip()
            return text if len(text) >= self.config.min_text_length else random.choice(self.fallback_templates[emotion])
        except Exception:
            return random.choice(self.fallback_templates[emotion])

    def parse_crema_filename(self, filename: str):
        parts = Path(filename).stem.split('_')
        return {'actor_id': parts[0], 'emotion': self.crema_emotions.get(parts[2]), 'dataset': 'crema_d'}

    def parse_ravdess_filename(self, filename: str):
        parts = Path(filename).stem.split('-')
        return {'actor_id': parts[6], 'emotion': self.ravdess_emotions.get(int(parts[2])), 'dataset': 'ravdess'}

    def process_enhanced_audio(self) -> List[Dict]:
        audio_data = []
        datasets = {
            "CREMA-D": (list(self.crema_path.glob("*.wav")) if self.crema_path.exists() else [], self.parse_crema_filename),
            "RAVDESS": (list(self.ravdess_path.glob("*.wav")) if self.ravdess_path.exists() else [], self.parse_ravdess_filename)
        }
        for name, (files, parser) in datasets.items():
            if not files:
                print(f"Warning: No files found for {name} at {getattr(self, name.lower().replace('-', '_') + '_path')}. Skipping.")
                continue
            for audio_file in tqdm(files, desc=f"Processing {name}"):
                metadata = parser(audio_file.name)
                if metadata and metadata['emotion']:
                    text = self.enhanced_transcribe(str(audio_file), metadata['emotion'])
                    audio_data.append({'text': text, 'emotion': metadata['emotion'], 'dataset': metadata['dataset'], 'data_type': 'audio', 'actor_id': metadata['actor_id'], 'id': str(uuid.uuid4())})
        return audio_data

class EnhancedDataProcessor:
    def __init__(self, config: EnhancedConfig):
        self.config = config
        self.synthetic_generator = EnhancedSyntheticGenerator(config)
        self.audio_processor = EnhancedAudioProcessor(config)

    def process_and_get_splits(self) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        Replicates the exact data processing pipeline from training to get
        identical data splits.
        """
        # Step 1: Generate synthetic data to advance the random state.
        # The output is not used, but this step is CRITICAL for reproducibility.
        _ = self.synthetic_generator.generate_enhanced_synthetic()

        # Step 2: Process audio data.
        audio_data = self.audio_processor.process_enhanced_audio()
        if not audio_data:
            raise RuntimeError("CRITICAL ERROR: No audio data could be processed.")

        # Step 3: Shuffle and split the audio data. The random state is now correct.
        audio_by_emotion = defaultdict(list)
        for item in audio_data: audio_by_emotion[item['emotion']].append(item)

        audio_train, audio_test, audio_val = [], [], []
        for emotion, items in audio_by_emotion.items():
            random.shuffle(items)
            n_test = max(1, int(len(items) * self.config.audio_test_ratio))
            n_val = max(1, int(len(items) * self.config.audio_val_ratio))
            audio_test.extend(items[:n_test])
            audio_val.extend(items[n_test:n_test + n_val])
            audio_train.extend(items[n_test + n_val:])

        return audio_train, audio_test, audio_val

# --- Core Reporting Functions ---

def load_model_for_inference(model_path: str, device: str):
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found at: {model_path}.")
    print(f"Loading model from {model_path}...")
    checkpoint = torch.load(model_path, map_location=torch.device(device), weights_only=False)
    config = checkpoint['config']
    config.device = device
    model = EnhancedModel(config)

    # Step 1: Load the full state dict first. This ensures all layers, including
    # frozen ones and batch norm stats, are initialized correctly from the checkpoint.
    model.load_state_dict(checkpoint['model_state_dict'])

    # Step 2: If EMA is available, overwrite the trainable parameters with the
    #         averaged weights from the EMA shadow dictionary for better performance.
    if config.use_ema and 'ema_state_dict' in checkpoint:
        print("Applying EMA (Exponential Moving Average) weights...")

        # Get the current model's state_dict
        model_dict = model.state_dict()

        # Update it with the EMA weights. This will only overwrite the keys present in the EMA dict.
        model_dict.update(checkpoint['ema_state_dict'])

        # Load the updated state_dict back into the model
        model.load_state_dict(model_dict)
    else:
        print("Using standard model weights.")

    model.to(device)
    model.eval()
    print("Model loaded successfully.")
    return model, config

def evaluate_model(model: EnhancedModel, data_loader: DataLoader, config: EnhancedConfig):
    all_preds, all_labels = [], []
    print("Evaluating model on the test set...")
    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Model Evaluation"):
            input_ids = batch['input_ids'].to(config.device)
            attention_mask = batch['attention_mask'].to(config.device)
            labels = batch['labels'].to(config.device)
            outputs = model(input_ids, attention_mask)
            preds = torch.argmax(outputs['logits'], dim=-1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    return all_preds, all_labels

def plot_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label'); plt.ylabel('True Label'); plt.title('Confusion Matrix')
    output_filename = 'confusion_matrix.png'
    plt.savefig(output_filename)
    print(f"\nConfusion matrix plot saved as '{output_filename}'")
    plt.show()

def generate_report(model_path: str = "best_enhanced_model.pt"):
    try:
        if not (WHISPER_AVAILABLE and LIBROSA_AVAILABLE):
             print("\nReport generation cancelled due to missing dependencies.")
             return

        device = "cuda" if torch.cuda.is_available() else "cpu"
        model, config = load_model_for_inference(model_path, device)

        print("\nReplicating exact training data pipeline to get test set...")
        data_processor = EnhancedDataProcessor(config)
        _, test_data, _ = data_processor.process_and_get_splits()
        print(f"Test data regenerated successfully: {len(test_data)} samples.")

        test_dataset = EnhancedDataset(test_data, model.tokenizer, config)
        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)

        y_pred, y_true = evaluate_model(model, test_loader, config)

        print("\n--- Emotion Detection Model Performance Report ---")
        print(f"\nOverall Accuracy: {accuracy_score(y_true, y_pred):.4f}")
        print(f"Macro-Averaged Precision: {precision_score(y_true, y_pred, average='macro', zero_division=0):.4f}")
        print(f"Macro-Averaged Recall: {recall_score(y_true, y_pred, average='macro', zero_division=0):.4f}")
        print("\nClassification Report:")
        print(classification_report(y_true, y_pred, target_names=config.emotion_classes, zero_division=0, digits=4))

        plot_confusion_matrix(y_true, y_pred, config.emotion_classes)

    except FileNotFoundError as e:
        print(f"Error: {e}")
    except Exception as e:
        import traceback
        print(f"An unexpected error occurred: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    generate_report()